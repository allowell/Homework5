---
title: "Homework 5 - Programming in Base R"
format: pdf
editor: visual
---

## Task 1: Conceptual Questions

### What is the purpose of using cross-validation when fitting a random forest model?
The purpose of using CV when fitting a random forest model is so that the parameters are tested on unseen data. CV allows us to find the ideal parameter that has been tested fully on data it hasn't seen. It allows for random forests to be a better model of the overall population since regular tree models can change drastically based on adding an extra observation, but with CV is is more robust.

### Describe the bagged tree algorithm.
The bagged tree algorithm is the simple version of random forests, and it involves using bootstrap sampling and doing the tree on each new random sample. This allows for a reduction in the variance of the model.

### What is meant by a general linear model?
A general linear model is a model that is linear in the parameters, and includes simple linear regression, multiple linear regression, and others. 

### When fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?
Adding an interaction term allows for the effect of one predictor to change depending on another predictor. It allows the model to capture interactions between parameters outside of just the main effects.

### Why do we split our data into a training and test set?
We split our data into training and testing set to evaluate how the model does on data that it hasn't seen yet. Since we are interested in how well our models predict, training the model on the training data and then testing it on the test data allows us to evaluate how well it predicts on unseen data.

## Task 2: Data Prep

### Packages and Data

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(caret)
library(tidymodels)
library(yardstick)
library(rsample)
library(glmnet)

heart_data <- as_tibble(read_csv("heart.csv"))
summary(heart_data)
```
Heart Disease is a numeric variable in R, as we can see from the above summary. However, this is a binary variable, so this is incorrect. It needs to be a categorical variable.

Now, creating the new data
```{r}
new_heart <- heart_data |>
  mutate(HeartDisease_bin = factor(HeartDisease, levels = c(0,1))) |>
  select(-c(ST_Slope, HeartDisease))
```

## Task 3: EDA

EDA Plot
```{r}
ggplot(new_heart, aes(x = MaxHR, y = Age, color = HeartDisease_bin)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Age by Maximum Heart Rate and Heart Disease Binary Predictor",
       x = "Max Heart Rate",
       y = "Age",
       color = "Heart Disease") +
  #Found colors using color brewer colorblind friendly palette
  scale_colour_manual(values = c("#7570b3", "#d95f02"), labels = c("No", "Yes"))
```
Based on the visual evidence, it looks like an interaction model is more appropriate because there are visible differences between the slope and intercept of the linear regression lines for Heart Disease = No versus Heart Disease = Yes.

## Task 4: Testing and Training

Splitting data into training and testing
```{r}
set.seed(101)

heart_split <- initial_split(new_heart, prop = 0.8)
train <- training(heart_split)
test <- testing(heart_split)
```

## Task 5: OLS and LASSO

Fitting Interaction Model
```{r}
#Interaction Model
ols_mlr <- lm(Age ~ MaxHR*HeartDisease_bin, data = train)

#Summary
summary(ols_mlr)

#RMSE of test data
ols_rmse <- ols_mlr |>
  predict(test) |>
  rmse_vec(truth = test$Age)
ols_rmse
```

LASSO
```{r}
LASSO_recipe <- recipe(Age ~ MaxHR + HeartDisease_bin, data = new_heart) |>
  step_dummy(HeartDisease_bin) |>
  step_normalize(MaxHR) |>
  step_interact( ~ MaxHR:starts_with("HeartDisease_bin_"))
LASSO_recipe
```

Selecting best model
```{r}
#Create folds
heart_folds <- vfold_cv(train, 10)

#Define model
LASSO_spec <- linear_reg(penalty = tune(), mixture = 1) |>
  set_engine("glmnet")

#Create Workflow
LASSO_wkf <- workflow() |>
  add_recipe(LASSO_recipe) |>
  add_model(LASSO_spec)
LASSO_wkf
```

Fit the model
```{r}
LASSO_grid <- LASSO_wkf |>
  tune_grid(resamples = heart_folds,
            grid = grid_regular(penalty(), levels = 200))
```

Pick the best
```{r}
lowest_rmse <- LASSO_grid |>
  select_best(metric = "rmse")

LASSO_final <- LASSO_wkf |>
  finalize_workflow(lowest_rmse) |>
  fit(train)
tidy(LASSO_final)
```

Without looking at the RMSE calculations, I would expect them to be about the same. This is because the penalty for the best LASSO model is very small for all the parameters, so the model isn't very different that the original model.

Comparing RMSE
```{r}
#RMSE for our best LASSO model
lasso_rmse <- LASSO_final |>
  predict(test) |>
  pull() |>
  rmse_vec(truth = test$Age)

#Compare to OLS
lasso_rmse
ols_rmse
```
The RMSEs are roughly the same even though the parameters are different because they are both predicting the new data well, despite not being the exact same model type.


## Task 6: Logistic Regression

```{r}
#Making the first model just the original basic one with interaction
LR1_model <- recipe(HeartDisease_bin ~ Age + MaxHR, data = new_heart) |>
  step_normalize(all_numeric())
LR2_model <- recipe(HeartDisease_bin ~ Age + RestingBP + Cholesterol + MaxHR, 
                 data = new_heart) |>
  step_normalize(all_numeric())

LR_spec <- logistic_reg() |>
  set_engine("glm")

#Setting up workflows
LR1_wkf <- workflow() |>
  add_recipe(LR1_model) |>
  add_model(LR_spec)
LR2_wkf <- workflow() |>
  add_recipe(LR2_model) |>
  add_model(LR_spec)
```

Fit using CV folds and collect metrics
```{r}
LR1_fit <- LR1_wkf |>
  fit_resamples(heart_folds, metrics = metric_set(accuracy, mn_log_loss))
LR2_fit <- LR2_wkf |>
  fit_resamples(heart_folds, metrics = metric_set(accuracy, mn_log_loss))

rbind(LR1_fit |> collect_metrics(),
      LR2_fit |> collect_metrics()) |>
  mutate(Model = c("Model 1", "Model 1", "Model 2", "Model 2")) |>
  select(Model, everything())
```
The second model, which has more predictors, is my model of choice because it has a higher accuracy and lower log loss than the other model. 

Final fit
```{r}
LR_train_fit <- LR2_wkf |> fit(train)
predictions <- predict(LR_train_fit, test) |> pull()
confusionMatrix(data = predictions, reference = test$HeartDisease_bin)
```
The model does fairly well on the testing data, as we can see from the above confusion matrix. There are 21 false negatives and 24 false positives from the testing data. We also obtained sensitivity = 0.7447 and specificity = 0.7667 from the confusionMatrix() function above. The sensitivity value means that we were 74.47% accurate in predicting true positives (values that were predicted positive and were positive), and the specificity value means that we were 76.67% accurate in predicting true negatives (values that were predicted negative and were negative).








